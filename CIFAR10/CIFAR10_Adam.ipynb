{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Batch Norm Layer- custom\n"
      ],
      "metadata": {
        "id": "23XymPl06uQ4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CKHgwrFzVPG"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "class BatchNormLayer(Layer):\n",
        "    def __init__(self, epsilon=0.01, alpha=0.5, nonlinearity=None, **kwargs):\n",
        "        super(BatchNormLayer, self).__init__(**kwargs)\n",
        "        self.epsilon = epsilon\n",
        "        self.alpha = alpha\n",
        "        self.nonlinearity = nonlinearity\n",
        "        self.beta = None\n",
        "        self.gamma = None\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(BatchNormLayer, self).build(input_shape)\n",
        "        axes = list(range(len(input_shape)))\n",
        "        shape = [1] * len(input_shape)\n",
        "        shape[-1] = input_shape[-1]\n",
        "        broadcast = [False] * len(input_shape)\n",
        "         # Initialize learnable parameters\n",
        "        self.beta = self.add_weight(name='beta', shape=shape,\n",
        "                                    initializer='zeros', trainable=True)\n",
        "        self.gamma = self.add_weight(name='gamma', shape=shape,\n",
        "                                     initializer='ones', trainable=True)\n",
        "        self.mean = self.add_weight(name='mean', shape=shape,\n",
        "                                    initializer=initializers.Constant(0),\n",
        "                                    trainable=False)\n",
        "        self.std = self.add_weight(name='std', shape=shape,\n",
        "                                   initializer=initializers.Constant(1),\n",
        "                                   trainable=False)\n",
        "\n",
        "        self.axes = axes\n",
        "        self.broadcast = broadcast\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        mean = K.mean(inputs, axis=self.axes, keepdims=True)\n",
        "        std = K.std(inputs, axis=self.axes, keepdims=True)\n",
        "        # Update moving averages for mean and standard deviation\n",
        "        self.add_update([(self.mean, (1 - self.alpha) * self.mean + self.alpha * mean),\n",
        "                         (self.std, (1 - self.alpha) * self.std + self.alpha * std)])\n",
        "        # Normalize inputs using batch normalization formula\n",
        "        normalized = (inputs - mean) * (self.gamma / (std + self.epsilon)) + self.beta\n",
        "        return normalized if self.nonlinearity is None else self.nonlinearity(normalized)\n",
        "# Function to apply Batch Normalization to a layer\n",
        "def batch_norm(layer):\n",
        "    nonlinearity = getattr(layer, 'activation', None)\n",
        "    if nonlinearity is not None:\n",
        "        layer.activation = None\n",
        "    if hasattr(layer, 'bias'):\n",
        "        layer.bias = None\n",
        "    return BatchNormLayer()(layer)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binary Connect layer- with custom DenseLayer and utility functions\n"
      ],
      "metadata": {
        "id": "pCB25RS-63CT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "#   Hard Sigmoid Function\n",
        "def hard_sigmoid(x):\n",
        "    return tf.clip_by_value((x + 1.0) / 2.0, 0, 1)\n",
        "# BinaryConnect weight binarization function\n",
        "def binarization(W, H, binary=True, deterministic=False, stochastic=False, seed=None):\n",
        "    if not binary or (deterministic and stochastic):\n",
        "        return W\n",
        "    else:\n",
        "        Wb = hard_sigmoid(W / H)\n",
        "\n",
        "        if stochastic:\n",
        "          # Apply random binomial sampling for stochastic BinaryConnect\n",
        "            Wb = tf.dtypes.cast(\n",
        "                tf.random.stateless_binomial(shape=tf.shape(W), seed=seed, counts=1, probs=Wb),\n",
        "                tf.float32\n",
        "            )\n",
        "        else:\n",
        "            # Deterministic rounding for deterministic BinaryConnect\n",
        "            Wb = tf.round(Wb)\n",
        "        # Mapping binary values to original weight range\n",
        "        Wb = tf.dtypes.cast(tf.where(Wb > 0, H, -H), tf.float32)\n",
        "        return Wb\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Custom DenseLayer with BinaryConnect\n",
        "class DenseLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, units, binary=True, stochastic=True, H=1.0, W_LR_scale=\"Glorot\", **kwargs):\n",
        "        super(DenseLayer, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.binary = binary\n",
        "        self.stochastic = stochastic\n",
        "        self.H = H\n",
        "        self.W_LR_scale = W_LR_scale\n",
        "        self.seed = tf.constant([42, 42], dtype=tf.int32)  # Set your desired seed values\n",
        "        self.W = None  # Initialize W = None\n",
        "\n",
        "        if W_LR_scale == \"Glorot\":\n",
        "            self.W_LR_scale = None  # Set to None initially\n",
        "        elif isinstance(W_LR_scale, str) and W_LR_scale.lower() == \"none\":\n",
        "            self.W_LR_scale = 1.0  # or any default value you prefer\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if self.W is None:\n",
        "            input_dim = tf.TensorShape(input_shape[-1]).as_list()[0]\n",
        "            self.units = self.units if self.units is not None else 1\n",
        "            # Initialize binary weights using RandomUniform distribution\n",
        "            self.W = self.add_weight(\n",
        "                name='kernel',\n",
        "                shape=(input_dim, self.units),\n",
        "                initializer=tf.initializers.RandomUniform(-self.H, self.H),\n",
        "                trainable=True\n",
        "            )\n",
        "\n",
        "            if self.W_LR_scale is None:\n",
        "                self.W_LR_scale = 1.0 / np.sqrt(1.5 / (self.units + input_dim))\n",
        "\n",
        "        super(DenseLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs, training=None, **kwargs):\n",
        "        # Apply BinaryConnect binarization to weights\n",
        "        self.Wb = binarization(self.W, self.H, self.binary, not training, self.stochastic, self.seed)\n",
        "        Wr = self.W\n",
        "        self.W = self.Wb\n",
        "        output = tf.matmul(inputs, self.W)\n",
        "        self.W = Wr\n",
        "        return output\n",
        "\n",
        "# compute gradients for BinaryConnect layers\n",
        "def compute_grads(loss, network):\n",
        "    layers = network.layers\n",
        "    grads = []\n",
        "    for layer in layers:\n",
        "        params = [var for var in model.trainable_variables if 'binary' in var.name.lower()]\n",
        "        for param in params:\n",
        "            grads.append(tf.gradients(loss, params)[0])\n",
        "    return grads\n",
        "# apply clipping and scaling to BinaryConnect layer updates\n",
        "def clipping_scaling(updates, model):\n",
        "    clipped_updates = {}\n",
        "\n",
        "    for layer in model.layers:\n",
        "        if isinstance(layer, DenseLayer):\n",
        "            lr_scaled_update = layer.W_LR_scale * (layer.Wb - layer.W)\n",
        "            clipped_update = tf.clip_by_value(layer.W + lr_scaled_update, -layer.H, layer.H)\n",
        "            # Use layer.W.ref() as the key\n",
        "            clipped_updates[layer.W.ref()] = clipped_update\n",
        "\n",
        "    return clipped_updates\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZgQKbX9_6oyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tw6Ugw_fT-Vm"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(1234)  # for reproducibility\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.utils import shuffle\n",
        "import tensorflow_datasets as tfds\n",
        "import pickle\n",
        "import gzip\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialise all the parameters\n",
        "    # BN parameters\n",
        "    batch_size = 50\n",
        "    print(\"batch_size = \" + str(batch_size))\n",
        "    # alpha is the exponential moving average factor\n",
        "    alpha = 0.1\n",
        "    print(\"alpha = \" + str(alpha))\n",
        "    epsilon = 1e-4\n",
        "    print(\"epsilon = \" + str(epsilon))\n",
        "\n",
        "    # Training parameters\n",
        "    num_epochs = 10\n",
        "    print(\"num_epochs = \" + str(num_epochs))\n",
        "\n",
        "    # Dropout parameters\n",
        "    dropout_in = 0.  # 0. means no dropout\n",
        "    print(\"dropout_in = \" + str(dropout_in))\n",
        "    dropout_hidden = 0.\n",
        "    print(\"dropout_hidden = \" + str(dropout_hidden))\n",
        "\n",
        "    # BinaryConnect\n",
        "    binary = True\n",
        "    print(\"binary = \" + str(binary))\n",
        "    stochastic = True # change to false for deterministic setting\n",
        "    print(\"stochastic = \" + str(stochastic))\n",
        "    # (-H,+H) are the two binary values\n",
        "    # H = \"Glorot\"\n",
        "    H = 1.\n",
        "    print(\"H = \" + str(H))\n",
        "    # W_LR_scale = 1.\n",
        "    W_LR_scale = \"Glorot\"  # \"Glorot\" means we are using the coefficients from Glorot's paper\n",
        "    print(\"W_LR_scale = \" + str(W_LR_scale))\n",
        "\n",
        "    # Decaying LR\n",
        "    LR_start = 0.003\n",
        "    print(\"LR_start = \" + str(LR_start))\n",
        "    LR_fin = 0.000002\n",
        "    print(\"LR_fin = \" + str(LR_fin))\n",
        "    LR_decay = (LR_fin / LR_start) ** (1. / num_epochs)\n",
        "    print(\"LR_decay = \" + str(LR_decay))\n",
        "\n",
        "\n",
        "\n",
        "    # Function to perform ZCA whitening\n",
        "\n",
        "\n",
        "    def zca_whiten(data):\n",
        "        data = tf.cast(data, dtype=tf.float32)  # Convert to float32\n",
        "\n",
        "        data_flat = tf.reshape(data, (tf.shape(data)[0], -1))\n",
        "\n",
        "        # PCA is performed first\n",
        "        pca = PCA(whiten=True)\n",
        "        pca.fit(data_flat.numpy())\n",
        "\n",
        "        # Apply ZCA whitening\n",
        "        zca_matrix = np.dot(pca.components_.T, np.diag(1.0 / np.sqrt(pca.explained_variance_ + 1e-5)))\n",
        "        whitened_data_flat = tf.linalg.matmul(data_flat, zca_matrix)\n",
        "\n",
        "        # Correctly reshape the whitened data\n",
        "        whitened_data = tf.reshape(whitened_data_flat, tf.shape(data))\n",
        "\n",
        "        return whitened_data.numpy().astype(np.float32)\n",
        "\n",
        "    # Load CIFAR-10 dataset\n",
        "    (train_set, train_labels), (test_set, test_labels) = cifar10.load_data()\n",
        "\n",
        "    # Apply ZCA whitening\n",
        "    train_set = zca_whiten(train_set)\n",
        "    test_set = zca_whiten(test_set)\n",
        "    #convert to float32\n",
        "    train_labels_hinge = np.float32(train_labels)\n",
        "    test_labels_hinge = np.float32(test_labels)\n",
        "    # perform one hot encoding\n",
        "    train_labels = np.float32(np.eye(10)[train_labels])\n",
        "    test_labels = np.float32(np.eye(10)[test_labels])\n",
        "    #perform operations for hinge loss\n",
        "    train_labels = 2 * train_labels - 1.\n",
        "    test_labels= 2 * test_labels - 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfv57V6A2Z68"
      },
      "outputs": [],
      "source": [
        "    import numpy as np\n",
        "    #alter the dimensions of labels to make it two-dimensional\n",
        "    train_labels = np.squeeze(train_labels)\n",
        "    test_labels = np.squeeze(test_labels)\n",
        "    print(train_set.shape)\n",
        "    print(train_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (32, 32,3)\n",
        "# Define a custom layer for binary convolution\n",
        "class BinaryConv2DLayer(Layer):\n",
        "    def __init__(self, filters, kernel_size, binary=True, stochastic=True, H=1.0, W_LR_scale=\"Glorot\", **kwargs):\n",
        "        super(BinaryConv2DLayer, self).__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.binary = binary\n",
        "        self.stochastic = stochastic\n",
        "        self.H = H\n",
        "        self.W_LR_scale = W_LR_scale\n",
        "        seed_value = seed  # You can use any desired seed value\n",
        "        self.rng = tf.random.Generator.from_seed(seed_value)\n",
        "    def build(self, input_shape):\n",
        "        # Create a binary weight matrix and initialize it using Glorot uniform initialization\n",
        "        self.kernel = self.add_weight(\n",
        "            name='kernel',\n",
        "            shape=(self.kernel_size[0], self.kernel_size[1], input_shape[-1], self.filters),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True\n",
        "        )\n",
        "        super(BinaryConv2DLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs, training=None, **kwargs):\n",
        "\n",
        "        # Use the generator for other random operations as needed\n",
        "        rand_int = self.rng.uniform(shape=(), minval=1, maxval=2147462579, dtype=tf.int32)\n",
        "\n",
        "        # Apply binarization to the weight matrix\n",
        "        Wb = binarization(self.kernel, self.H, self.binary, not training, seed = rand_int)\n",
        "        Wr = self.kernel\n",
        "        self.kernel = Wb\n",
        "        # Perform binary convolution\n",
        "        output = tf.nn.conv2d(inputs, Wb, strides=[1, 1, 1, 1], padding='SAME')\n",
        "        # Restore the original weight matrix for subsequent layers\n",
        "        self.kernel = Wr\n",
        "        return output\n",
        "#  Define a custom model using the BinaryConv2DLayer\n",
        "class MyModel(Model):\n",
        "    def __init__(self, input_shape, binary, stochastic, H, W_LR_scale, **kwargs):\n",
        "        super(MyModel, self).__init__(**kwargs)\n",
        "       # Define layers for the model using BinaryConv2DLayer\n",
        "        self.binary_conv1 = BinaryConv2DLayer(filters=128, kernel_size=(3, 3), binary=binary, stochastic=stochastic, H=H, W_LR_scale=W_LR_scale, name='binary_conv1')\n",
        "        self.batch_norm = BatchNormLayer(epsilon=epsilon, alpha=alpha)\n",
        "        self.max_pool = layers.MaxPooling2D(pool_size=(2, 2), padding='same')\n",
        "        self.batch_norm1= BatchNormLayer(epsilon=epsilon, alpha=alpha)\n",
        "        self.batch_norm2 = BatchNormLayer(epsilon=epsilon, alpha=alpha)\n",
        "        self.batch_norm3 = BatchNormLayer(epsilon=epsilon, alpha=alpha)\n",
        "\n",
        "        self.binary_conv2 = BinaryConv2DLayer(filters=256, kernel_size=(3, 3), binary=binary, stochastic=stochastic, H=H, W_LR_scale=W_LR_scale, name='binary_conv2')\n",
        "\n",
        "        self.binary_conv3 = BinaryConv2DLayer(filters=512, kernel_size=(3, 3), binary=binary, stochastic=stochastic, H=H, W_LR_scale=W_LR_scale, name='binary_conv3')\n",
        "\n",
        "    # Model forward pass\n",
        "    def call(self, inputs, training=None, **kwargs):\n",
        "        x = self.binary_conv1(inputs)\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.max_pool(x)\n",
        "        x = self.batch_norm1(x)\n",
        "\n",
        "        x = self.binary_conv2(inputs)\n",
        "        x = self.batch_norm2(x)\n",
        "        x = self.binary_conv2(inputs)\n",
        "        x = self.max_pool(x)\n",
        "        x = self.batch_norm2(x)\n",
        "\n",
        "        x = self.binary_conv3(inputs)\n",
        "        x = self.batch_norm3(x)\n",
        "        x = self.binary_conv3(inputs)\n",
        "        x = self.max_pool(x)\n",
        "        x = self.batch_norm3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Usage\n",
        "input_layer = Input(shape=input_shape, name='input')\n",
        "my_model = MyModel(input_shape=input_shape, binary=binary, stochastic=stochastic, H=H, W_LR_scale=W_LR_scale)\n",
        "output = my_model(input_layer)\n",
        "output = Flatten()(output)\n",
        "output = DenseLayer(1024, binary=binary, stochastic=stochastic, H=H, W_LR_scale=W_LR_scale, name='binary_dense1')(output)\n",
        "output = BatchNormLayer(epsilon=epsilon, alpha=alpha, nonlinearity=None)(output)\n",
        "output = DenseLayer(1024, binary=binary, stochastic=stochastic, H=H, W_LR_scale=W_LR_scale, name='binary_dense2')(output)\n",
        "output = BatchNormLayer(epsilon=epsilon, alpha=alpha, nonlinearity=None)(output)\n",
        "output = DenseLayer(10, binary=binary, stochastic=stochastic, H=H, W_LR_scale=W_LR_scale, name='binary_dense3')(output)\n",
        "output = BatchNormLayer(epsilon=epsilon, alpha=alpha, nonlinearity=None)(output)\n",
        "\n",
        "# Create the final model\n",
        "model = Model(inputs=input_layer, outputs=output)\n"
      ],
      "metadata": {
        "id": "aBvRAKzY_Vwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Metrics"
      ],
      "metadata": {
        "id": "x4oM0deu_uBF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIoHluBmQ61H"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "from tensorflow.keras.layers import Input, Dropout, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import MeanSquaredError, Hinge\n",
        "from tensorflow.keras.metrics import Mean, SparseCategoricalAccuracy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Input, Dropout, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "#loss function\n",
        "loss_object = Hinge()\n",
        "\n",
        "#metrics\n",
        "train_loss = Mean(name=\"train_loss\")\n",
        "val_loss = Mean(name=\"val_loss\")\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=LR_start )\n",
        "\n",
        "@tf.function\n",
        "def train_step(inputs, targets, LR):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        predictions = model(inputs, training=True)\n",
        "        loss = tf.reduce_mean(tf.square(tf.maximum(0., 1. - targets * predictions)))\n",
        "\n",
        "\n",
        "    if binary:\n",
        "      #compute binary parameters\n",
        "        binary_params = [var for var in model.trainable_weights if 'binary' in var.name.lower()]\n",
        "\n",
        "        # Watch binary_params\n",
        "        for var in binary_params:\n",
        "            tape.watch(var)\n",
        "\n",
        "        grads = tape.gradient(loss, binary_params)\n",
        "\n",
        "        # Filter out None gradients\n",
        "        grads_and_vars = zip(grads, binary_params)\n",
        "        grads_and_vars = [(grad, var) for grad, var in grads_and_vars if grad is not None]\n",
        "        #use optimizer\n",
        "        updates_binary = optimizer.apply_gradients(grads_and_vars)\n",
        "        updates_binary = clipping_scaling(updates_binary, model)\n",
        "        #update other parameters\n",
        "        non_binary_params = [var for var in model.trainable_variables if 'binary' not in var.name.lower()]\n",
        "\n",
        "        gradients_other = tape.gradient(loss, non_binary_params)\n",
        "\n",
        "        updates_other = optimizer.apply_gradients(zip(gradients_other, non_binary_params))\n",
        "\n",
        "\n",
        "    else:\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        trainable_vars = [var for var in model.trainable_variables if 'binary' in var.name.lower()]\n",
        "\n",
        "        # Print shapes for debugging\n",
        "        for var, grad in zip(trainable_vars, gradients):\n",
        "            print(f\"Trainable Variable: {var.name}, Shape: {var.shape}, Gradient: {grad.shape}\")\n",
        "\n",
        "        updates_final = optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def val_step(inputs, targets):\n",
        "    predictions = model(inputs, training=False)\n",
        "    # Cast targets to float32 to match the data type of predictions\n",
        "    targets = tf.cast(targets, dtype=tf.float32)\n",
        "    loss = loss_object(targets, predictions)\n",
        "    return loss\n",
        "\n",
        "# Training loop\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(0, len(train_set), batch_size):\n",
        "        x_batch = train_set[i:i + batch_size]\n",
        "        y_batch = train_labels[i:i + batch_size]\n",
        "\n",
        "\n",
        "        loss = train_step(x_batch, y_batch, LR_start)\n",
        "        train_loss(loss)\n",
        "\n",
        "    for i in range(0, len(test_set), batch_size):\n",
        "        x_val_batch = test_set[i:i + batch_size]\n",
        "        y_val_batch = test_labels[i:i + batch_size]\n",
        "\n",
        "\n",
        "        loss= val_step(x_val_batch, y_val_batch)\n",
        "        val_loss(loss)\n",
        "    train_losses.append(train_loss.result().numpy())\n",
        "    val_losses.append(val_loss.result().numpy())\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {train_loss.result()}, Validation Loss: {val_loss.result()}')\n",
        "\n",
        "\n",
        "    train_loss.reset_states()\n",
        "    val_loss.reset_states()\n",
        "\n",
        "\n",
        "# Plotting the training and validation losses\n",
        "df_adam = pd.DataFrame({'Epoch': range(1, num_epochs + 1), 'Training Loss': train_losses, 'Validation Loss': val_losses})\n",
        "\n",
        "# Plotting the training and validation losses\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss over Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D88iEbtZ3jHY"
      },
      "outputs": [],
      "source": [
        "df_adam.to_csv('d_adam.csv')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}