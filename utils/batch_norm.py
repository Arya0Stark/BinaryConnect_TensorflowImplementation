# -*- coding: utf-8 -*-
"""Batch_Norm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i2Ja5yS0w9QSqn2Ki2-Oqv_XvuZq2xnA
"""

import tensorflow as tf
from tensorflow.keras import layers
import numpy as np
import time
import tensorflow as tf
from tensorflow.keras.layers import Layer
from tensorflow.keras import initializers
from tensorflow.keras import backend as K
from __future__ import print_function

import sys
import os
import time
import tensorflow as tf
from tensorflow.keras.layers import Input, Dropout, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import MeanSquaredError, Hinge
from tensorflow.keras.metrics import Mean, SparseCategoricalAccuracy
import numpy as np
fashion_mnist = tf.keras.datasets.fashion_mnist
import pandas as pd
from tensorflow.keras.layers import Input, Dropout, Flatten
from tensorflow.keras.models import Model

import numpy as np
np.random.seed(1234)  # for reproducibility

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
from sklearn.utils import shuffle

from collections import OrderedDict

class BatchNormLayer(Layer):
    def __init__(self, axes=None, epsilon=0.01, alpha=0.5, nonlinearity=None, **kwargs):
        super(BatchNormLayer, self).__init__(**kwargs)
        self.axes = axes
        self.epsilon = epsilon
        self.alpha = alpha
        self.nonlinearity = nonlinearity
        self.beta = None
        self.gamma = None

    def build(self, input_shape):
        shape = list(input_shape)
        broadcast = [False] * len(shape)
        if self.axes is None:
            # default: normalize over all but the second axis
            self.axes = (0,) + tuple(range(2, len(shape)))
        elif isinstance(self.axes, int):
            self.axes = (self.axes,)
        for axis in self.axes:
            shape[axis] = 1
            broadcast[axis] = True
        if any(size is None for size in shape):
            raise ValueError("BatchNormLayer needs specified input sizes for "
                             "all dimensions/axes not normalized over.")
        # Initialize learnable parameters
        self.beta = self.add_weight(name='beta', shape=shape,
                                    initializer='zeros', trainable=True)
        self.gamma = self.add_weight(name='gamma', shape=shape,
                                     initializer='ones', trainable=True)
        self.mean = self.add_weight(name='mean', shape=shape,
                                    initializer=initializers.Constant(0),
                                    trainable=False)
        self.std = self.add_weight(name='std', shape=shape,
                                   initializer=initializers.Constant(1),
                                   trainable=False)
        super(BatchNormLayer, self).build(input_shape)

    def call(self, inputs, **kwargs):
        mean = K.mean(inputs, axis=self.axes, keepdims=True)
        std = K.std(inputs, axis=self.axes, keepdims=True)
        # Update moving averages for mean and standard deviation
        self.add_update([(self.mean, (1 - self.alpha) * self.mean + self.alpha * mean),
                         (self.std, (1 - self.alpha) * self.std + self.alpha * std)])

        # Normalize inputs using batch normalization formula
        normalized = (inputs - mean) * (self.gamma / (std + self.epsilon)) + self.beta
        return normalized if self.nonlinearity is None else self.nonlinearity(normalized)

# Function to apply Batch Normalization to a layer
def batch_norm(layer):
    nonlinearity = getattr(layer, 'activation', None)
    if nonlinearity is not None:
        layer.activation = None
    if hasattr(layer, 'bias'):
        layer.bias = None
    return BatchNormLayer()(layer)