{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "loe8DCxQjNgo",
        "3ZAxyeJFjSvz",
        "JEaYOlZwj1I9"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import backend as K\n",
        "from __future__ import print_function\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dropout, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import MeanSquaredError, Hinge\n",
        "from tensorflow.keras.metrics import Mean, SparseCategoricalAccuracy\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers import Input, Dropout, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(1234)  # for reproducibility\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from collections import OrderedDict"
      ],
      "metadata": {
        "id": "Xprw6oepjun3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch Norm Layer"
      ],
      "metadata": {
        "id": "loe8DCxQjNgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchNormLayer(Layer):\n",
        "    def __init__(self, axes=None, epsilon=0.01, alpha=0.5, nonlinearity=None, **kwargs):\n",
        "        super(BatchNormLayer, self).__init__(**kwargs)\n",
        "        self.axes = axes\n",
        "        self.epsilon = epsilon\n",
        "        self.alpha = alpha\n",
        "        self.nonlinearity = nonlinearity\n",
        "        self.beta = None\n",
        "        self.gamma = None\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        shape = list(input_shape)\n",
        "        broadcast = [False] * len(shape)\n",
        "        if self.axes is None:\n",
        "            # default: normalize over all but the second axis\n",
        "            self.axes = (0,) + tuple(range(2, len(shape)))\n",
        "        elif isinstance(self.axes, int):\n",
        "            self.axes = (self.axes,)\n",
        "        for axis in self.axes:\n",
        "            shape[axis] = 1\n",
        "            broadcast[axis] = True\n",
        "        if any(size is None for size in shape):\n",
        "            raise ValueError(\"BatchNormLayer needs specified input sizes for \"\n",
        "                             \"all dimensions/axes not normalized over.\")\n",
        "        # Initialize learnable parameters\n",
        "        self.beta = self.add_weight(name='beta', shape=shape,\n",
        "                                    initializer='zeros', trainable=True)\n",
        "        self.gamma = self.add_weight(name='gamma', shape=shape,\n",
        "                                     initializer='ones', trainable=True)\n",
        "        self.mean = self.add_weight(name='mean', shape=shape,\n",
        "                                    initializer=initializers.Constant(0),\n",
        "                                    trainable=False)\n",
        "        self.std = self.add_weight(name='std', shape=shape,\n",
        "                                   initializer=initializers.Constant(1),\n",
        "                                   trainable=False)\n",
        "        super(BatchNormLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        mean = K.mean(inputs, axis=self.axes, keepdims=True)\n",
        "        std = K.std(inputs, axis=self.axes, keepdims=True)\n",
        "        # Update moving averages for mean and standard deviation\n",
        "        self.add_update([(self.mean, (1 - self.alpha) * self.mean + self.alpha * mean),\n",
        "                         (self.std, (1 - self.alpha) * self.std + self.alpha * std)])\n",
        "\n",
        "        # Normalize inputs using batch normalization formula\n",
        "        normalized = (inputs - mean) * (self.gamma / (std + self.epsilon)) + self.beta\n",
        "        return normalized if self.nonlinearity is None else self.nonlinearity(normalized)\n",
        "\n",
        "# Function to apply Batch Normalization to a layer\n",
        "def batch_norm(layer):\n",
        "    nonlinearity = getattr(layer, 'activation', None)\n",
        "    if nonlinearity is not None:\n",
        "        layer.activation = None\n",
        "    if hasattr(layer, 'bias'):\n",
        "        layer.bias = None\n",
        "    return BatchNormLayer()(layer)"
      ],
      "metadata": {
        "id": "q2P-AU9fjPx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binary Connect Layer"
      ],
      "metadata": {
        "id": "3ZAxyeJFjSvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Hard sigmoid function for binary weights\n",
        "def hard_sigmoid(x):\n",
        "    return tf.clip_by_value((x + 1.0) / 2.0, 0, 1)\n",
        "\n",
        "# BinaryConnect weight binarization function\n",
        "def binarization(W, H, binary=True, deterministic=False, stochastic=False, seed=None):\n",
        "    if not binary or (deterministic and stochastic):\n",
        "        return W\n",
        "    else:\n",
        "        Wb = hard_sigmoid(W / H)\n",
        "\n",
        "        if stochastic:\n",
        "            # Apply random binomial sampling for stochastic BinaryConnect\n",
        "            Wb = tf.dtypes.cast(\n",
        "                tf.random.stateless_binomial(shape=tf.shape(W), seed=seed, counts=1, probs=Wb),\n",
        "                tf.float32\n",
        "            )\n",
        "        else:\n",
        "            # Deterministic rounding for deterministic BinaryConnect\n",
        "            Wb = tf.round(Wb)\n",
        "        # Mapping binary values to original weight range\n",
        "        Wb = tf.dtypes.cast(tf.where(Wb > 0, H, -H), tf.float32)\n",
        "        return Wb\n",
        "\n",
        "# Custom DenseLayer with BinaryConnect\n",
        "class DenseLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, units, binary=True, stochastic=True, H=1.0, W_LR_scale=\"Glorot\", **kwargs):\n",
        "        super(DenseLayer, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.binary = binary\n",
        "        self.stochastic = stochastic\n",
        "        self.H = H\n",
        "        self.W_LR_scale = W_LR_scale\n",
        "        self.seed = tf.constant([42, 42], dtype=tf.int32)  # Set your desired seed values\n",
        "        self.W = None  # Initialize W to None\n",
        "\n",
        "        if W_LR_scale == \"Glorot\":\n",
        "            self.W_LR_scale = None  # Set to None initially\n",
        "        elif isinstance(W_LR_scale, str) and W_LR_scale.lower() == \"none\":\n",
        "            self.W_LR_scale = 1.0  # or any default value you prefer\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if self.W is None:\n",
        "            input_dim = tf.TensorShape(input_shape[-1]).as_list()[0]\n",
        "            self.units = self.units if self.units is not None else 1\n",
        "            # Initialize binary weights using RandomUniform distribution\n",
        "            self.W = self.add_weight(\n",
        "                name='kernel',\n",
        "                shape=(input_dim, self.units),\n",
        "                initializer=tf.initializers.RandomUniform(-self.H, self.H),\n",
        "                trainable=True\n",
        "            )\n",
        "\n",
        "            if self.W_LR_scale is None:\n",
        "                self.W_LR_scale = 1.0 / np.sqrt(1.5 / (self.units + input_dim))\n",
        "\n",
        "        super(DenseLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs, training=None, **kwargs):\n",
        "        # Apply BinaryConnect binarization to weights\n",
        "        self.Wb = binarization(self.W, self.H, self.binary, not training, self.stochastic, self.seed)\n",
        "        Wr = self.W\n",
        "        self.W = self.Wb\n",
        "        output = tf.matmul(inputs, self.W)\n",
        "        self.W = Wr\n",
        "        return output\n",
        "\n",
        "\n",
        "# compute gradients for BinaryConnect layers\n",
        "def compute_grads(loss, network):\n",
        "    grads = []\n",
        "    for layer in network.layers:\n",
        "        if hasattr(layer, 'Wb'):\n",
        "            grads.append(tf.gradients(loss, layer.Wb)[0])\n",
        "    return grads\n",
        "\n",
        "# apply clipping and scaling to BinaryConnect layer updates\n",
        "def clipping_scaling(updates, model):\n",
        "    clipped_updates = {}\n",
        "\n",
        "    for layer in model.layers:\n",
        "        if isinstance(layer, DenseLayer):\n",
        "            lr_scaled_update = layer.W_LR_scale * (layer.Wb - layer.W)\n",
        "            clipped_update = tf.clip_by_value(layer.W + lr_scaled_update, -layer.H, layer.H)\n",
        "            clipped_updates[layer.W.ref()] = clipped_update\n",
        "\n",
        "    return clipped_updates\n"
      ],
      "metadata": {
        "id": "xJIVyRD4jUya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fix Parameters and Load data"
      ],
      "metadata": {
        "id": "JEaYOlZwj1I9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# BN parameters\n",
        "batch_size = 100\n",
        "print(\"batch_size = \" + str(batch_size))\n",
        "# alpha is the exponential moving average factor\n",
        "alpha = 0.15\n",
        "print(\"alpha = \" + str(alpha))\n",
        "epsilon = 1e-4\n",
        "print(\"epsilon = \" + str(epsilon))\n",
        "\n",
        "# MLP parameters\n",
        "num_units = 2048\n",
        "print(\"num_units = \" + str(num_units))\n",
        "n_hidden_layers = 3\n",
        "print(\"n_hidden_layers = \" + str(n_hidden_layers))\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 10\n",
        "print(\"num_epochs = \" + str(num_epochs))\n",
        "\n",
        "# Dropout parameters\n",
        "dropout_in = 0. # no dropout\n",
        "print(\"dropout_in = \" + str(dropout_in))\n",
        "dropout_hidden = 0.\n",
        "print(\"dropout_hidden = \" + str(dropout_hidden))\n",
        "\n",
        "# BinaryConnect\n",
        "binary = True\n",
        "print(\"binary = \" + str(binary))\n",
        "stochastic = True    # change to false for deterministic setting\n",
        "print(\"stochastic = \" + str(stochastic))\n",
        "H = 1.\n",
        "print(\"H = \" + str(H))\n",
        "# W_LR_scale = 1.\n",
        "W_LR_scale = \"Glorot\"  # using the coefficients from Glorot's paper\n",
        "print(\"W_LR_scale = \" + str(W_LR_scale))\n",
        "\n",
        "# Decaying LR\n",
        "LR_start = 0.001\n",
        "print(\"LR_start = \" + str(LR_start))\n",
        "LR_fin = 0.000003\n",
        "print(\"LR_fin = \" + str(LR_fin))\n",
        "LR_decay = (LR_fin / LR_start) ** (1. / num_epochs)\n",
        "print(\"LR_decay = \" + str(LR_decay))\n",
        "\n",
        "print('Loading MNIST dataset...')\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
        "\n",
        "\n",
        "X_train = X_train.reshape(-1, 1, 28, 28).astype(np.float32)\n",
        "X_test = X_test.reshape(-1, 1, 28, 28).astype(np.float32)\n",
        "\n",
        "# flatten targets\n",
        "y_train = np.hstack(y_train)\n",
        "y_test = np.hstack(y_test)\n",
        "\n",
        "# Onehot the targets\n",
        "y_train = np.float32(np.eye(10)[y_train])\n",
        "y_test = np.float32(np.eye(10)[y_test])\n",
        "\n",
        "# for hinge loss\n",
        "y_train = 2 * y_train - 1.\n",
        "y_test = 2 * y_test - 1.\n",
        "print(X_train.shape)\n",
        "\n",
        "print('Building the MLP...')\n",
        "\n",
        "\n",
        "# Prepare TensorFlow variables for inputs and targets\n",
        "input_shape = (1, 28, 28)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e353dd22-aaf1-4f02-d5c8-391fc31a7d4d",
        "id": "XRJ962ruj1I-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_size = 100\n",
            "alpha = 0.15\n",
            "epsilon = 0.0001\n",
            "num_units = 2048\n",
            "n_hidden_layers = 3\n",
            "num_epochs = 10\n",
            "dropout_in = 0.0\n",
            "dropout_hidden = 0.5\n",
            "binary = True\n",
            "stochastic = True\n",
            "H = 1.0\n",
            "W_LR_scale = Glorot\n",
            "LR_start = 0.001\n",
            "LR_fin = 3e-06\n",
            "LR_decay = 0.5593866859813431\n",
            "Loading MNIST dataset...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 2s 0us/step\n",
            "(30000, 1, 28, 28)\n",
            "Building the MLP...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building and training model"
      ],
      "metadata": {
        "id": "t-CNI4ZGjn57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## BUILDING THE MLP MODEL #########################################################\n",
        "def build_model(input_shape, num_units, n_hidden_layers, binary, stochastic, H, dropout_in, dropout_hidden):\n",
        "    input_layer = Input(shape=input_shape, name=\"input\")\n",
        "    x = Dropout(rate=dropout_in)(input_layer)\n",
        "\n",
        "    for i in range(n_hidden_layers):\n",
        "        dense_layer = DenseLayer(units=num_units, binary=binary, stochastic=stochastic, H=H, name=f'dense_layer_{i}')(x)\n",
        "        x = BatchNormLayer(epsilon=epsilon, alpha=alpha)(dense_layer)\n",
        "        x = Dropout(rate=dropout_hidden)(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    output_layer = DenseLayer(units=10, binary=binary, stochastic=stochastic, H=H, name='output_dense_layer')(x)\n",
        "    output = BatchNormLayer(epsilon=epsilon, alpha=alpha, nonlinearity=None)(output_layer)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "# Create the model\n",
        "model = build_model((1, 28, 28), num_units, n_hidden_layers, binary, stochastic, H, dropout_in, dropout_hidden)\n",
        "\n",
        "# Loss function\n",
        "loss_object = Hinge()\n",
        "\n",
        "\n",
        "\n",
        "# Metrics\n",
        "train_loss = Mean(name=\"train_loss\")\n",
        "val_loss = Mean(name=\"val_loss\")\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=LR_start, nesterov = True)\n",
        "\n",
        "@tf.function\n",
        "def train_step(inputs, targets, LR):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inputs, training=True)\n",
        "        loss = loss_object(targets, predictions)\n",
        "\n",
        "    if binary:\n",
        "        grads = compute_grads(loss, model)\n",
        "        binary_params = [var for var in model.trainable_variables if 'dense' in var.name.lower()]\n",
        "        updates_binary = optimizer.apply_gradients(zip(grads, binary_params))\n",
        "        updates_binary = clipping_scaling(updates_binary, model)\n",
        "\n",
        "        non_binary_params = [var for var in model.trainable_variables if 'dense' not in var.name.lower()]\n",
        "        gradients_other = tape.gradient(loss, non_binary_params)\n",
        "        updates_other = optimizer.apply_gradients(zip(gradients_other, non_binary_params))\n",
        "\n",
        "        tf.executing_eagerly() and updates_binary\n",
        "        tf.executing_eagerly() and updates_other\n",
        "\n",
        "    else:\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        trainable_vars = [var for var in model.trainable_variables if 'dense' in var.name.lower()]  # Modify this based on your layer naming\n",
        "        updates_final = optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        tf.executing_eagerly() and updates_final\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def val_step(inputs, targets):\n",
        "    predictions = model(inputs, training=False)\n",
        "    # Cast targets to float32 to match the data type of predictions\n",
        "    targets = tf.cast(targets, dtype=tf.float32)\n",
        "    loss = loss_object(targets, predictions)\n",
        "    return loss\n",
        "\n",
        "# Training loop\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Lists to store the training and validation losses for each epoch\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "num_epochs = 10\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        x_batch = X_train[i:i + batch_size]\n",
        "        y_batch = y_train[i:i + batch_size]\n",
        "\n",
        "        loss = train_step(x_batch, y_batch, LR_start)\n",
        "        train_loss(loss)\n",
        "\n",
        "    for i in range(0, len(X_test), batch_size):\n",
        "        x_val_batch = X_test[i:i + batch_size]\n",
        "        y_val_batch = y_test[i:i + batch_size]\n",
        "\n",
        "        loss = val_step(x_val_batch, y_val_batch)\n",
        "        val_loss(loss)\n",
        "\n",
        "    # Append the training and validation losses for the current epoch\n",
        "    train_losses.append(train_loss.result().numpy())\n",
        "    val_losses.append(val_loss.result().numpy())\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {train_loss.result()}, Validation Loss: {val_loss.result()}')\n",
        "\n",
        "    # Reset the metrics for the next epoch\n",
        "    train_loss.reset_states()\n",
        "    val_loss.reset_states()\n",
        "\n",
        "# Creating a DataFrame\n",
        "df_sgd_n = pd.DataFrame({'Epoch': range(1, num_epochs + 1), 'Training Loss': train_losses, 'Validation Loss': val_losses})\n",
        "\n",
        "# Plotting the training and validation losses\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss over Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sfhxFXQej28_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sgdn.to_csv('dr_sgdn.csv')"
      ],
      "metadata": {
        "id": "aMC7B5zNj28_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}